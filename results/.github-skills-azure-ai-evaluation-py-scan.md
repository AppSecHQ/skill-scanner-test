# Agent Skill Security Scan Report

**Skill:** azure-ai-evaluation-py
**Directory:** /workspace/skills/microsoft-agent-skills/.github/skills/azure-ai-evaluation-py
**Status:** [OK] SAFE
**Max Severity:** MEDIUM
**Scan Duration:** 31.99s
**Timestamp:** 2026-02-05T13:28:30.727792

## Summary

- **Total Findings:** 1
- **Critical:** 0
- **High:** 0
- **Medium:** 1
- **Low:** 0
- **Info:** 0

## Findings

### MEDIUM Severity

#### [MEDIUM] Hardcoded API Key in Model Configuration Examples

**Severity:** MEDIUM
**Category:** data_exfiltration
**Rule ID:** LLM_DATA_EXFILTRATION
**Location:** SKILL.md

**Description:** The SKILL.md instructions and reference files contain examples showing hardcoded API keys in model configuration dictionaries. While these are example placeholders, they establish an insecure pattern that users may follow, potentially leading to credential exposure in actual implementations. The instructions show 'api_key': os.environ['AZURE_OPENAI_API_KEY'] which is secure, but also show direct string assignment patterns like 'api_key': '<your-api-key>' in multiple locations.

## Analyzers

The following analyzers were used:

- static_analyzer
- behavioral_analyzer
- llm_analyzer
- trigger_analyzer
- meta_analyzer
