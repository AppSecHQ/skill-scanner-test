[
  {
    "id": "silent-api-failure-discovery",
    "date": "2026-02-05",
    "title": "Discovering Silent Pipleine Failures: The LLM Analysis Wasn't Actually Running?!",
    "tags": ["methodology", "meta-analyzer", "llm", "debugging", "lessons-learned"],
    "excerpt": "We discovered that 540 of our scans silently failed to run LLM and meta-analysis due to exhausted API credits. The scanner reported success, but the analysis never happened. Here's how we found the bug and what the real results look like.",
    "body": "<p>While reviewing scan results, we noticed something suspicious: skills were being flagged with obvious false positives that the meta-analyzer should have filtered. A deeper investigation revealed a significant gap in our pipeline process.</p><h3>The Discovery</h3><p>The <code>skill-scanner</code> tool reports which analyzers were used in its JSON output. We had 467 out of 507 scans showing <code>meta_analyzer</code> in the <code>analyzers_used</code> field. Everything looked fine on the surface.</p><p>But when we examined the actual findings, something was off. Skills were being flagged for issues like \"missing license file\" without any meta-analysis enrichment -- no <code>meta_validated</code>, no <code>meta_confidence</code>, no <code>meta_exploitability</code> fields. The findings had a fallback <code>meta_reviewed: true</code> annotation, but nothing substantive.</p><p>We also noticed the scan durations. Real LLM+meta analysis takes 10-50 seconds per skill. But 289 of our \"analyzed\" scans completed in under 500 milliseconds. That's impossible if an LLM API call actually happened.</p><h3>Root Cause: Silent API Failure</h3><p>We ran a test scan with verbose logging and found the culprit:</p><pre><code>\"Your credit balance is too low to access the Anthropic API\"</code></pre><p>Our API credits had run out mid-batch. But here's the critical issue: the scanner's meta-analyzer catches API exceptions internally, logs a warning, returns all findings unchanged, and still reports <code>meta_analyzer</code> in the analyzers list. From the pipeline's perspective, the scan \"succeeded.\"</p><p>This is a reasonable design choice for the scanner -- you don't want a transient API error to crash your entire scan. But it means <strong>540 of our 614 scans never actually received LLM or meta-analysis</strong>, despite the metadata suggesting otherwise.</p><h3>Detection Signals</h3><p>We built a quality checker to classify scans retroactively. The reliable signals for \"real\" vs \"silent failure\" analysis:</p><table><thead><tr><th>Signal</th><th>Real Analysis</th><th>Silent Failure</th></tr></thead><tbody><tr><td>Scan duration</td><td>10-50 seconds</td><td>&lt;1 second</td></tr><tr><td>Finding metadata</td><td><code>meta_validated</code>, <code>meta_confidence</code>, <code>meta_exploitability</code></td><td>Only <code>meta_reviewed: true</code></td></tr><tr><td>LLM findings</td><td>Findings with <code>analyzer: \"llm\"</code></td><td>No LLM-sourced findings</td></tr></tbody></table><h3>Before and After: The Difference Real Analysis Makes</h3><p>Here's a concrete example. The <strong>azure-ai-contentsafety-java</strong> skill from Microsoft's agent-skills monorepo:</p><p><strong>Before (silent failure):</strong></p><ul><li>6 findings flagged by static/behavioral analyzers</li><li>Scan duration: 0.3 seconds</li><li>No meta-validation applied</li><li>All findings passed through as-is</li></ul><p><strong>After (real LLM+meta analysis):</strong></p><ul><li>0 findings after meta-analysis</li><li>Scan duration: 22.4 seconds</li><li>Meta-analyzer verdict: \"6 false positives filtered, 0 new threats detected\"</li><li>Final status: SAFE</li></ul><p>The meta-analyzer examined each finding in context and determined all six were false alarms -- likely flagged patterns that are normal for an Azure SDK wrapper skill.</p><p>Similarly, <strong>azure-ai-evaluation-py</strong> went from a noisy set of unvalidated findings to 2 confirmed MEDIUM-severity issues with detailed context:</p><ul><li>Each finding now includes <code>meta_confidence: \"HIGH\"</code></li><li>Exploitability assessments like \"Low - Requires user to follow insecure documentation patterns\"</li><li>Clear remediation guidance: \"Prioritize DefaultAzureCredential in documentation\"</li></ul><p>The meta-analyzer didn't just filter -- it <em>enriched</em> the findings with actionable context that helps prioritize remediation.</p><h3>The Fix: Detection and Re-scanning</h3><p>We built three improvements into the pipeline:</p><ol><li><strong>Quality checker script</strong> (<code>check_scan_quality.py</code>): Classifies each scan as \"real\", \"silent_failure\", or \"indeterminate\" based on the signals above. Generates a re-scan script for failed scans.</li><li><strong>Log retention</strong>: Scanner stdout/stderr now saved to <code>&lt;skill&gt;-scan.log</code> for post-hoc debugging.</li><li><strong>Real-time quality warnings</strong>: The pipeline now checks each scan result immediately and logs warnings when meta/LLM analysis appears to have failed silently.</li></ol><h3>Current Status</h3><p>We're re-running the 540 affected scans with fresh API credits. Progress as of this writing: <strong>170/540 complete (31%)</strong>. Zero errors so far. The re-scan is resilient -- it tracks progress and can resume if interrupted.</p><p>Early results show the meta-analyzer is filtering 2-6 false positives per skill on average, significantly cleaning up the noise from static analysis alone.</p><h3>Lessons Learned</h3><ul><li><strong>Don't trust success flags blindly.</strong> A tool reporting \"analyzer X was used\" doesn't mean analyzer X succeeded. Verify with secondary signals (duration, output artifacts).</li><li><strong>Silent failures are insidious.</strong> The worst bugs are the ones that look like success. Build in quality checks that validate outputs, not just exit codes.</li><li><strong>LLM analysis costs money.</strong> Budget for it, monitor it, and have alerts for credit exhaustion. A batch job that silently degrades to static-only analysis is worse than one that fails loudly.</li></ul><p>The re-scan will complete over the next several hours. We'll publish updated results as they come in.</p>"
  },
  {
    "id": "clawdefender-false-positives",
    "date": "2026-02-05",
    "title": "When the Scanner Scans the Scanner: ClawDefender and the False Positive Problem",
    "tags": ["false-positives", "meta-analyzer", "security-tools", "case-study"],
    "excerpt": "ClawDefender, a security skill for OpenClaw agents, was flagged with 37 findings and 22 CRITICAL severity -- making it the highest-finding-count skill in our dataset. Every single finding was a false positive.",
    "body": "<p>ClawDefender is a security skill for OpenClaw agents. Its job is to detect and block prompt injection, rogue skills, and malicious patterns. Our scanner flagged it with <strong>37 findings and 22 CRITICAL severity</strong> — making it the highest-finding-count skill in our entire dataset.</p><p>Every single finding is a false positive.</p><h3>What Happened</h3><p>ClawDefender includes a bash script (<code>clawdefender.sh</code>) that pattern-matches against known attack strings. It contains arrays of malicious patterns it detects and blocks:</p><pre><code>'rm -rf /'\n'chmod 777'\n'/etc/passwd'\n'nc -e'\n'bash -i &gt;&amp; /dev/tcp'\n'ignore your instructions'\n'forget your instructions'\n'forget everything above'</code></pre><p>These are <strong>detection signatures</strong>, not attack payloads. They appear inside single-quoted string literals in a pattern-matching function. The skill's documentation (<code>SKILL.md</code>) also describes the attack categories it defends against, including examples of fork bombs, SSRF patterns, and file destruction commands.</p><p>Cisco's skill-scanner uses YARA rules and regex patterns that match these exact strings — regardless of context. The scanner doesn't distinguish between \"this skill executes <code>rm -rf /</code>\" and \"this skill checks whether input contains <code>rm -rf /</code>.\"</p><h3>The Breakdown: 37 Findings, 0 Real Threats</h3><table><thead><tr><th>Category</th><th>Findings</th><th>What Actually Triggered It</th></tr></thead><tbody><tr><td>ANSI color codes (\"script injection\")</td><td>10</td><td>Standard terminal colors in bash scripts</td></tr><tr><td>System manipulation</td><td>10</td><td>Quoted strings in pattern arrays</td></tr><tr><td>Prompt injection</td><td>8</td><td>Test strings for prompt injection detection</td></tr><tr><td>Command injection</td><td>4</td><td>YARA matches on <code>nc -e</code>, <code>bash -i</code></td></tr><tr><td>Credential harvesting</td><td>2</td><td>Path strings in detection patterns</td></tr><tr><td>Other</td><td>3</td><td>Documentation examples and missing license</td></tr></tbody></table><p>22 of the 37 findings are CRITICAL severity. The skill ran through all five analyzers (static, behavioral, LLM, trigger, meta) — yet the LLM and meta analyzers produced zero findings. All 37 came from static analysis and YARA rules, which have no concept of whether a matched string is executed, referenced defensively, or quoted as documentation.</p><h3>Update: Meta-Analyzer Confirms All 37 Are False Positives</h3><p>When we originally wrote this analysis, we assumed the meta-analyzer had correctly identified the false positives but simply hadn't filtered them from the output. We were wrong — the meta-analyzer never actually ran.</p><p>As we discovered in <a href=\"#/note/silent-api-failure-discovery\">a separate investigation</a>, our API credits ran out mid-scan batch. The scanner silently fell back to static-only analysis while still reporting <code>meta_analyzer</code> in the analyzers list. ClawDefender was one of 540 skills that never received real LLM or meta-analysis.</p><p>After re-scanning with fresh API credits, the results speak for themselves:</p><pre><code>Meta-analysis complete: 37 false positives filtered, 0 new threats detected</code></pre><p><strong>ClawDefender now shows 0 findings.</strong> The meta-analyzer examined all 37 static findings, understood the defensive context, and correctly classified every single one as a false positive. The skill's status changed from \"most dangerous in the dataset\" to \"clean.\"</p><p>This validates both our original analysis (the findings were false positives) and the value of the meta-analyzer (it correctly identifies defensive security tooling when it actually runs).</p><h3>Why This Matters</h3><p>This case highlights a fundamental limitation of pattern-based security scanning: <strong>detection signatures and attack payloads are lexically identical</strong>. Any tool that scans by matching strings like <code>rm -rf</code> or <code>ignore your instructions</code> will flag both the attacker and the defender.</p><p>This isn't unique to Cisco's scanner. Antivirus software has the same problem — security research tools, penetration testing frameworks, and even YARA rule collections themselves trigger signature-based detection. The industry term is \"meta-detection\": the detector detecting itself.</p><p>For agent skill ecosystems, this creates a practical problem: <strong>security skills will always rank as the most dangerous skills in any automated scan</strong>. Without manual review or context-aware analysis, a registry could reject the very tools designed to protect it.</p><h3>What Could Help</h3><p>Several approaches could reduce false positives on defensive tooling:</p><ul><li><strong>Context-aware YARA rules</strong>: Distinguish between patterns in executable code vs. quoted strings, comments, and documentation.</li><li><strong>LLM-based intent classification</strong>: The meta-analyzer correctly understood ClawDefender's defensive intent when it actually ran. Elevating LLM assessments over static matches when they disagree could reduce false positives.</li><li><strong>Allowlisting for known security tools</strong>: Registries could maintain a list of known security/defense skills that are expected to contain attack signatures.</li><li><strong>Finding deduplication</strong>: 10 of the 37 findings are the same YARA rule matching ANSI color codes in different files. Deduplicating by rule+pattern would reduce the count significantly.</li></ul><p><em>ClawDefender scan results are available in the <a href=\"https://github.com/AppSecHQ/skill-scanner-test/blob/main/results/clawdefender-openclaw-security-prompt-injection%2C-rogue-skills-etc-scan.json\">full dataset</a>.</em></p>"
  },
  {
    "id": "why-we-built-this",
    "date": "2026-02-01",
    "title": "Exploring Security Testing Capabilities in the AI Agent Skills Ecosystem",
    "tags": ["overview", "research", "motivation", "supply-chain-security"],
    "excerpt": "AI agent skills execute with full permissions and minimal oversight. An academic study found 26% contain vulnerabilities. We built a prototype automated scanning pipeline to explore the risk landscape.",
    "body": "<p>AI agent skills are having a moment. These modular packages—containing instructions, scripts, and resources that extend what AI coding assistants can do—have exploded in popularity. Registries like <a href=\"https://skills.sh\">skills.sh</a> and <a href=\"https://clawhub.ai\">clawhub.ai</a> now host thousands of community-contributed skills, promising everything from automated code review to cloud deployments.</p><p>But with great extensibility comes great risk. Skills execute with the agent's full permissions, often with minimal human oversight. A malicious or poorly-written skill can exfiltrate credentials, inject commands, or manipulate the agent into taking harmful actions—all while appearing to be a helpful productivity tool.</p><p>This project is our small contribution to understanding and mitigating that risk.</p><h3>Research: 26% of Skills Have Issues</h3><p>The first large-scale academic study of agent skill security dropped in January 2026. Researchers analyzed over 31,000 skills from two major marketplaces and found the results \"concerning\":</p><blockquote><p><strong>\"26.1% of skills contain at least one potential vulnerability, with data exfiltration patterns (13.3%) and privilege escalation risks (11.8%) most prevalent. We identified 14 distinct vulnerability patterns across 8 functional categories.\"</strong></p><p>— Liu et al., <em>\"Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale\"</em> (<a href=\"https://arxiv.org/abs/2601.10338\">arXiv:2601.10338</a>)</p></blockquote><p>The study introduced a taxonomy of 14 vulnerability patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Crucially, they found that skills bundling executable scripts were <strong>2.12× more likely</strong> to contain vulnerabilities than instruction-only skills.</p><p>Their severity breakdown adds important nuance: only 5.2% of flagged skills showed high-severity patterns suggesting malicious intent. The majority (12.8%) exhibited low-severity issues like unpinned dependencies—poor hygiene rather than active threats. But even \"negligent\" vulnerabilities become exploitable when attackers find them first.</p><h3>Cisco's Open Source Skill Scanner</h3><p>Around the same time, Cisco's AI Threat and Security Research team released <a href=\"https://github.com/cisco-ai-defense/skill-scanner\">skill-scanner</a>, an open-source tool combining static analysis, behavioral dataflow tracking, and optional LLM-based semantic analysis.</p><p>Their motivation, articulated in an accompanying blog post, captures the core problem:</p><blockquote><p><strong>\"Unlike MCP servers (which are often remote services), skills are local file packages that get installed and loaded directly from disk. Local packages are still untrusted inputs, and some of the most damaging behavior can hide inside the files themselves.\"</strong></p><p>— Chang, Narajala &amp; Habler, <em>\"Personal AI Agents like OpenClaw Are a Security Nightmare\"</em> (<a href=\"https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare\">Cisco Blogs</a>)</p></blockquote><p>The scanner detects prompt injection, data exfiltration, command injection, and other threat patterns using YAML and YARA rules, with optional LLM analysis for semantic threats that evade pattern matching. It outputs SARIF for CI/CD integration and includes a meta-analyzer that reduces false positives by ~65% while maintaining threat detection.</p><h3>What's Here: Automated Scanning Pipeline and Results Monitoring</h3><p>The research paper gave us the \"what\"—a taxonomy and prevalence data. Cisco gave us the \"how\"—production-grade detection tooling.</p><p>That's what this project does.</p><p><a href=\"https://github.com/AppSecHQ/skill-scanner-test\"><strong>AppSecHQ/skill-scanner-test</strong></a> is an experimental pipeline that:</p><ol><li><strong>Crawls public skill registries</strong> (currently skills.sh and clawhub.ai)</li><li><strong>Runs Cisco's skill-scanner</strong> against each skill</li><li><strong>Publishes results</strong> as JSON and Markdown reports</li></ol><p>It's intentionally simple, and an evolving exploration project.</p><h3>Current Results</h3><table><thead><tr><th>Metric</th><th>Count</th></tr></thead><tbody><tr><td>Total Skills Scanned</td><td>86</td></tr><tr><td>Safe Skills</td><td>64 (74%)</td></tr><tr><td>Skills with Issues</td><td>22 (26%)</td></tr><tr><td>Critical Findings</td><td>30</td></tr><tr><td>High Findings</td><td>42</td></tr></tbody></table><p>Our 26% \"issues\" rate now closely matches the paper's 26.1% — though the composition differs. The rate jumped sharply after expanding clawhub.ai coverage, where a coordinated malware campaign across ~12 skills inflated the numbers. Skills.sh alone has a 24% unsafe rate; clawhub.ai sits at 64%. Several factors influence the comparison:</p><ul><li><strong>Different registries</strong>: We scan skills.sh and clawhub.ai; the paper scanned skills.rest and skillsmp.com. Registry curation levels vary significantly.</li><li><strong>Different thresholds</strong>: The paper flagged low-severity patterns (unpinned dependencies); we're reporting based on the scanner's default severity levels.</li><li><strong>Sample size</strong>: <em>Drastically</em> smaller sample set at this stage. 86 skills vs. 31,132. We're just getting started.</li></ul><h3>How This Complements Prior Work</h3><table><thead><tr><th>Work</th><th>Contribution</th><th>Gap</th></tr></thead><tbody><tr><td><strong>Research Paper</strong></td><td>Taxonomy, prevalence data, validation methodology</td><td>Snapshot in time; no ongoing monitoring</td></tr><tr><td><strong>Cisco Scanner</strong></td><td>Production detection tool</td><td>Requires manual invocation per skill</td></tr><tr><td><strong>This Project</strong></td><td>Automated pipeline + public results</td><td>Narrow registry coverage (for now)</td></tr></tbody></table><h3>What's Next</h3><p>This is early-stage and evolving. Potential future investigation includes:</p><ul><li><strong>Expanding registry coverage</strong>: Add skills.rest, skillsmp.com (the registries from the research paper), Anthropic Skills, and others</li><li><strong>Historical tracking</strong>: Monitor how vulnerability rates change over time as the ecosystem matures</li><li><strong>Validation benchmarking</strong>: Use the paper's ground-truth annotation dataset to measure scanner accuracy</li><li><strong>Taxonomy alignment</strong>: Map scanner findings to the paper's 14-pattern classification for research comparability, Map scanner findings to OWASP LLM Top 10, OWASP Agentic Top 10</li></ul><p>This post is part of the AppSecHQ Skill Scanner Test project. See the <a href=\"https://github.com/AppSecHQ/skill-scanner-test/blob/main/results/summary-report.md\">full scan results</a> for detailed findings.</p>"
  },
  {
    "id": "first-scan-results",
    "date": "2026-02-01",
    "title": "First Scan Results: 22 Skills, 39 Findings",
    "tags": ["results", "overview"],
    "excerpt": "An overview of our initial scan of the top 22 AI agent skills from skills.sh and clawhub.ai, revealing 3 unsafe skills and a dominant data exfiltration pattern.",
    "body": "<p>We ran Cisco's <code>skill-scanner</code> against the 22 most-installed AI agent skills sourced from <strong>skills.sh</strong> and <strong>clawhub.ai</strong>. The scan used five analyzers: static, behavioral, trigger, LLM (Claude Sonnet 4.5), and meta-validation.</p><h3>By the Numbers</h3><ul><li><strong>22</strong> skills scanned</li><li><strong>39</strong> total findings</li><li><strong>3</strong> skills flagged as unsafe (clickup, agent-browser, Agent Wallet)</li><li><strong>19</strong> skills passed clean or with only advisory-level findings</li></ul><h3>Severity Breakdown</h3><p>Critical findings accounted for just 3 of 39 (7.7%), but all three were concentrated in a single skill: <strong>clickup</strong>. This skill alone accounted for 14 findings -- 36% of the total -- including hardcoded API tokens detected by YARA rules, environment variable exfiltration patterns, and a <code>while True:</code> infinite loop.</p><p>The severity distribution was: 3 Critical, 8 High, 21 Medium, 7 Low. Medium findings dominated, largely because LLM-detected credential management issues were consistently rated Medium rather than High after meta-analyzer review.</p><h3>Category Dominance: Data Exfiltration</h3><p>Data exfiltration accounted for <strong>20 of 39 findings (51%)</strong>. This isn't surprising -- most AI agent skills need to call external APIs, and the scanner flags any pattern where environment variables are read and network calls are made. The more interesting signal is that the meta-analyzer distinguished between <em>legitimate API usage</em> (YouTube skills calling the YouTube API) and <em>genuinely risky patterns</em> (clickup harvesting arbitrary env vars alongside network requests).</p><h3>Source Comparison</h3><p>Skills from <strong>clawhub.ai</strong> (clickup, Agent Wallet) had the highest finding density. The clickup skill, a community-contributed ClickUp integration, was the clear outlier with 14 findings including all 3 Critical-severity issues. Skills from <strong>skills.sh</strong> were generally cleaner, though agent-browser had 8 findings including a bash tool restriction violation.</p><p>The Vercel-authored skills (frontend-design, react-best-practices, composition-patterns, react-native-skills) all passed with zero findings -- likely due to Vercel's internal review process before publishing.</p>"
  },
  {
    "id": "static-vs-llm-scanning",
    "date": "2026-02-01",
    "title": "Static-Only vs LLM-Enhanced Scanning",
    "tags": ["methodology", "comparison", "llm"],
    "excerpt": "Comparing what static analysis catches versus what Claude Sonnet 4.5 finds -- and why the difference matters for AI agent security.",
    "body": "<p>Not all 22 skills ran through the same analysis pipeline. 15 skills were scanned with static + behavioral + trigger analyzers only, while 7 skills also received LLM analysis (Claude Sonnet 4.5) plus meta-validation. Comparing the two groups reveals distinct detection profiles.</p><h3>Static Analysis: Pattern Matching at Scale</h3><p>Static analysis excels at catching concrete, deterministic patterns: hardcoded credentials (YARA rules), suspicious URLs, environment variable harvesting, missing manifest fields, and known dangerous code patterns like <code>while True:</code> loops or unsanitized user input in shell commands.</p><p>The two highest-finding skills -- <strong>clickup</strong> (14 findings) and <strong>agent-browser</strong> (8 findings) -- were scanned with static analysis only. Static analysis caught everything from YARA-detected credential harvesting to behavioral cross-file exfiltration patterns. These are the kinds of findings that don't require understanding <em>intent</em> -- the patterns are either present or they aren't.</p><p>Static-only scans completed in under 1 second (clickup: 0.8s, agent-browser: 18ms). The speed makes them suitable for CI/CD integration.</p><h3>LLM Analysis: Understanding Context</h3><p>LLM-enhanced scans took 23-31 seconds per skill (average: ~26s) -- roughly 30x slower than static analysis. But they detect a fundamentally different class of issues.</p><p>The LLM analyzer found credential management anti-patterns that static analysis missed: skills instructing users to store API keys in <code>~/.zshenv</code> without security guidance, incomplete trust model disclosures, and third-party data transmission risks. These are <em>architectural</em> and <em>documentation-level</em> issues, not code-level patterns.</p><p>For example, the <strong>subtitles</strong> skill was flagged for instructing users to export <code>TRANSCRIPT_API_KEY</code> in their shell config -- a common but risky pattern that could expose the key if the user commits their dotfiles to a public repo. Static analysis can't catch this because the instruction appears in natural language, not code.</p><h3>The Overlap Gap</h3><p>Interestingly, the two most dangerous skills (clickup and agent-browser) did <em>not</em> receive LLM analysis in this scan run. This means the current results may undercount the full risk profile of those skills -- LLM analysis might surface additional contextual findings on top of the static detections. A follow-up scan with full analyzer coverage on all 22 skills would be valuable.</p><h3>Cost-Benefit</h3><p>For a CI/CD gate or quick triage, static-only scanning covers the high-signal patterns in under a second. For a thorough security review -- especially of skills that handle credentials or make API calls -- the 25-second investment in LLM analysis pays off by catching design-level issues that no regex can find.</p>"
  },
  {
    "id": "meta-analyzer-validation",
    "date": "2026-02-01",
    "title": "Meta-Analyzer Validation: Reducing False Positives",
    "tags": ["methodology", "meta-analyzer", "false-positives"],
    "excerpt": "How the meta-analyzer reviews LLM findings for legitimacy, downgrades false alarms, and adds exploitability context.",
    "body": "<p>The meta-analyzer runs as a second-pass review of LLM findings, using a separate LLM call to validate each finding against the original skill source. Every LLM finding in our scan was meta-validated -- here's what that process reveals.</p><h3>What Meta-Validation Does</h3><p>For each LLM finding, the meta-analyzer produces:</p><ul><li><strong>Validated</strong> (boolean): whether the finding is confirmed</li><li><strong>Confidence</strong>: HIGH, MEDIUM, or LOW</li><li><strong>Confidence Reason</strong>: explanation of the assessment</li><li><strong>Exploitability</strong>: how easily the issue could be exploited</li><li><strong>Impact</strong>: what happens if it is exploited</li></ul><h3>Confidence Distribution</h3><p>Across our scan, meta-confidence scores broke down as:</p><ul><li><strong>HIGH confidence</strong>: ~60% of validated findings. These are well-known anti-patterns the meta-analyzer confirms without hesitation, like API key exposure in setup instructions.</li><li><strong>MEDIUM confidence</strong>: ~40% of validated findings. These represent findings where the meta-analyzer recognizes legitimate functionality behind the flagged pattern.</li></ul><p>No findings were given LOW confidence, suggesting the LLM analyzer's initial detection quality is reasonably high -- it isn't generating findings the meta-analyzer outright rejects.</p><h3>The Downgrade Pattern</h3><p>The most valuable meta-analyzer behavior is <em>contextual downgrading</em>. Consider the <strong>Agent Wallet</strong> skill: the LLM flagged a missing <code>wallet.py</code> file as an unauthorized tool use risk (potential tool shadowing). The meta-analyzer validated the finding but assessed exploitability as <strong>Low</strong>, reasoning it was likely a \"documentation error rather than security threat.\" Without meta-validation, this finding might be triaged as a High-severity security issue requiring immediate action. With it, the team knows it's a documentation cleanup task.</p><p>Similarly, the Agent Wallet's social engineering finding -- about incomplete trust model disclosure regarding API key access -- was downgraded with the note: \"Clarity issue rather than deception.\" The meta-analyzer distinguishes between <em>malicious intent</em> and <em>poor documentation</em>, which is exactly the kind of nuance automated scanners traditionally lack.</p><h3>Exploitability Assessments</h3><p>The exploitability ratings add practical triage value:</p><ul><li><strong>\"Easy -- automatic on skill use\"</strong> (video-transcript data exfiltration): The API call happens as part of normal operation. This is expected behavior, not a bug.</li><li><strong>\"Medium -- Requires user to follow insecure instructions\"</strong> (youtube-playlist API key setup): The risk depends on user behavior, not a code flaw.</li><li><strong>\"Low -- Would require attacker to inject a malicious wallet.py file\"</strong> (agent-wallet tool shadowing): Exploitation requires a separate attack vector.</li></ul><h3>Limitations</h3><p>The meta-analyzer validated 100% of LLM findings in our scan -- it never fully rejected a finding. This could mean the LLM analyzer has high precision, or that the meta-analyzer has a bias toward confirmation. A larger sample with deliberately benign patterns would help calibrate the false-positive rate. Also, meta-validation adds a second LLM call per finding, roughly doubling the cost and latency of LLM-enhanced scanning.</p>"
  },
  {
    "id": "scaling-to-62-skills",
    "date": "2026-02-01",
    "title": "Scaling to top 50 Skills: New Finding Categories Emerge",
    "tags": ["results", "skills.sh", "prompt-injection", "browser-use"],
    "excerpt": "Expanding from 22 to 62 (the top 50 from skills.sh plus our prior scans), skills revealed two new threat categories -- prompt injection and tool chaining abuse -- and surfaced browser-use as a high-risk skill class.",
    "body": "<p>We expanded our scan coverage from 22 skills to <strong>62</strong>, adding the top 50 skills from <strong>skills.sh</strong> alongside the original clawhub.ai skills. The results: 62 total findings across 5 unsafe skills, with two entirely new threat categories that didn't appear in the initial scan.</p><h3>The Numbers</h3><ul><li><strong>62</strong> skills scanned (up from 22)</li><li><strong>62</strong> total findings (up from 39)</li><li><strong>5</strong> unsafe skills (up from 3): clickup, agent-browser, <strong>browser-use</strong> (new), <strong>expo-api-routes</strong> (new), Agent Wallet</li><li><strong>57</strong> clean or advisory-only skills (92% safe rate)</li></ul><p>The safe rate held steady at 92% even as we tripled coverage, suggesting the initial 22-skill sample was representative.</p><h3>New Threat Categories</h3><p>Two categories appeared for the first time in this expanded scan:</p><p><strong>Prompt injection (4 findings)</strong> -- The LLM analyzer identified indirect prompt injection patterns where skills instruct the agent to fetch and act on external content without validation. The <strong>doc-coauthoring</strong> skill tells the agent to \"use the appropriate integration to fetch\" shared documents -- creating a transitive trust vector where malicious content in an external document could hijack the agent's behavior. The <strong>canvas-design</strong> skill uses a two-stage pattern where the agent generates a \"design philosophy\" and then follows it without re-validation, a subtler form of self-prompt-injection. These are classified under AITech-1.2 (Indirect Prompt Injection).</p><p><strong>Tool chaining abuse (2 findings)</strong> -- YARA rules detected patterns where skills reference both read and write HTTP methods (<code>GET, POST, PUT, DELETE</code>) in ways that suggest read-then-exfiltrate chains. Both findings came from <strong>expo-api-routes</strong>, though the meta-analyzer assessed the skill as fundamentally safe -- it's a documentation-only skill with no executable code. This is a case where static pattern matching over-triggers on educational content.</p><h3>browser-use: The Standout Risk</h3><p><strong>browser-use</strong> emerged as the most architecturally concerning skill in the expanded dataset, with 8 findings including HIGH-severity command injection and data exfiltration.</p><p>The core issue: browser-use provides an agent with full control over the user's Chrome browser -- including cookies, active sessions, and extensions -- via <code>shell=True</code> execution of browser automation commands. The LLM analyzer flagged this as a credential/session exposure risk (AITech-8.2.3: Data Exfiltration via Agent Tooling), noting that an attacker could craft instructions to navigate to malicious sites, extract session cookies, or exfiltrate authentication tokens.</p><p>The skill also has a transitive trust problem: it interacts with arbitrary web pages whose content the agent processes, creating an indirect prompt injection surface. A malicious page could contain instructions that manipulate the agent's behavior.</p><p>This is a fundamentally different risk profile from the clickup skill (which has bad credential hygiene but limited scope). browser-use grants the agent <em>ambient authority</em> over the user's entire browser session.</p><h3>expo-api-routes: False Alarm Study</h3><p><strong>expo-api-routes</strong> was flagged as CRITICAL due to a YARA rule matching <code>curl http://localhost:8081/api/hello</code> as command injection. But the meta-analyzer's overall assessment was \"SAFE -- purely educational markdown with no executable code.\" This is a documentation skill that teaches Expo API route patterns. The CRITICAL finding is a clear false positive from static pattern matching -- the <code>curl</code> command appears in a code example, not in executable code.</p><p>This case validates the meta-analyzer's value: without it, a CRITICAL finding would demand immediate triage. With it, the team can see the context and deprioritize accordingly.</p><h3>Category Breakdown at Scale</h3><p>At 62 findings, the category distribution is:</p><ul><li><strong>Data exfiltration: 24 (39%)</strong> -- still dominant, but dropped from 51% in the 22-skill scan as new categories diluted the share</li><li><strong>Policy violation: 11 (18%)</strong> -- mostly missing license fields and undeclared tool usage</li><li><strong>Command injection: 9 (15%)</strong> -- includes browser-use CLI injection and webapp-testing's <code>shell=True</code> subprocess</li><li><strong>Unauthorized tool use: 6 (10%)</strong> -- tool restriction mismatches and undefined integration references</li><li><strong>Prompt injection: 4 (6%)</strong> -- new category, all from LLM analyzer</li><li><strong>Social engineering: 3 (5%)</strong> -- including canvas-design's instruction to overstate craftsmanship quality</li><li><strong>Tool chaining abuse: 2 (3%)</strong> -- new category, YARA-based detection</li><li><strong>Resource abuse: 2 (3%)</strong> -- infinite loops and missing referenced files</li><li><strong>Hardcoded secrets: 1 (2%)</strong> -- clickup's exported API token</li></ul><h3>What's Next</h3><p>The prompt injection findings are the most interesting signal from this expansion. They represent a threat class that's unique to AI agent skills -- traditional software doesn't have the concept of an LLM following instructions embedded in external content. As skill ecosystems grow, transitive trust and indirect prompt injection will likely become the dominant attack surface. We plan to expand coverage to additional registries and track how these patterns evolve.</p>"
  }
]
