#!/usr/bin/env python3
"""
Check scan quality and identify scans needing re-run with LLM/meta analysis.

Reads all scan result JSONs and classifies each scan's LLM and meta analysis
status. Generates a report and optionally a shell script to re-scan failures.

Usage:
    python scripts/check_scan_quality.py -r results
    python scripts/check_scan_quality.py -r results --generate-script scripts/rescan-failed.sh
"""

import argparse
import json
import logging
import sys
from pathlib import Path

SCRIPT_DIR = Path(__file__).parent
sys.path.insert(0, str(SCRIPT_DIR))

from pipeline_utils import LOG_FORMAT, LOG_DATE_FORMAT

logger = logging.getLogger(__name__)

# Statuses
REAL = "real"
SILENT_FAILURE = "silent_failure"
NOT_REQUESTED = "not_requested"
INDETERMINATE = "indeterminate"


def classify_scan(data: dict) -> dict:
    """
    Classify a scan result's LLM and meta analysis status.

    Returns dict with keys:
        meta_status: str
        llm_status: str
        needs_rescan: bool
        reasons: list[str]
    """
    analyzers = data.get("analyzers_used", [])
    findings = data.get("findings", [])
    duration = data.get("scan_duration_seconds", 0)

    has_meta_listed = "meta_analyzer" in analyzers
    has_llm_listed = "llm_analyzer" in analyzers

    has_meta_validated = any(
        "meta_validated" in f.get("metadata", {}) for f in findings
    )
    has_llm_finding = any(f.get("analyzer") == "llm" for f in findings)

    result = {"reasons": []}

    # --- Meta status ---
    if not has_meta_listed:
        result["meta_status"] = NOT_REQUESTED
        result["reasons"].append("meta_analyzer not in analyzers_used")
    elif not findings:
        if duration < 1.0:
            result["meta_status"] = SILENT_FAILURE
            result["reasons"].append("zero findings + fast scan (<1s)")
        else:
            result["meta_status"] = INDETERMINATE
    elif has_meta_validated:
        result["meta_status"] = REAL
    else:
        result["meta_status"] = SILENT_FAILURE
        result["reasons"].append("meta_reviewed=True but no meta_validated fields")

    # --- LLM status ---
    if not has_llm_listed:
        result["llm_status"] = NOT_REQUESTED
        result["reasons"].append("llm_analyzer not in analyzers_used")
    elif has_llm_finding:
        result["llm_status"] = REAL
    elif not findings:
        if duration < 1.0:
            result["llm_status"] = SILENT_FAILURE
            result["reasons"].append("zero findings + fast scan (<1s)")
        else:
            result["llm_status"] = INDETERMINATE
    elif duration < 1.0:
        result["llm_status"] = SILENT_FAILURE
        result["reasons"].append("llm_analyzer listed but no LLM findings + fast scan")
    else:
        result["llm_status"] = INDETERMINATE

    # --- Needs re-scan? ---
    # Only flag silent failures, not correctly-skipped analyzers.
    # Meta is legitimately skipped when there are 0 findings.
    # LLM not_requested means the scan was run without --use-llm.
    result["needs_rescan"] = (
        result["meta_status"] == SILENT_FAILURE
        or result["llm_status"] == SILENT_FAILURE
    )

    return result


def generate_rescan_script(skills: list[dict], output_path: Path,
                           scanner_path: str = ".venv/bin/skill-scanner") -> None:
    """Write a shell script that re-scans all failed skills.

    Features:
    - Continues on individual scan failures (no set -e)
    - Tracks progress with a counter file for resumability
    - Skips already-completed scans on restart
    - Reports success/failure counts at the end
    """
    lines = [
        "#!/usr/bin/env bash",
        "# Re-scan skills with failed LLM/meta analysis",
        "# Generated by check_scan_quality.py",
        "#",
        "# Features:",
        "#   - Continues past individual failures",
        "#   - Tracks progress for resumability",
        "#   - Run again to resume from where it stopped",
        "#   - Use --force to re-scan everything",
        "set -uo pipefail",
        "",
        f'SCANNER="${{SCANNER:-{scanner_path}}}"',
        'RESULTS_DIR="${RESULTS_DIR:-results}"',
        'PROGRESS_FILE="${RESULTS_DIR}/.rescan-progress"',
        'FORCE="${1:-}"',
        "",
        "# Counters",
        "TOTAL=0",
        "SKIPPED=0",
        "SUCCESS=0",
        "FAILED=0",
        "",
        "# Load completed scans from progress file",
        "declare -A COMPLETED",
        'if [[ -f "$PROGRESS_FILE" && "$FORCE" != "--force" ]]; then',
        '    while IFS= read -r stem; do',
        '        COMPLETED["$stem"]=1',
        '    done < "$PROGRESS_FILE"',
        '    echo "Loaded ${#COMPLETED[@]} completed scans from progress file"',
        "fi",
        "",
        f"echo 'Re-scanning {len(skills)} skills...'",
        "echo",
        "",
        "scan_skill() {",
        '    local path="$1"',
        '    local stem="$2"',
        '    local name="$3"',
        "    ",
        '    TOTAL=$((TOTAL + 1))',
        "    ",
        "    # Skip if already completed",
        '    if [[ -n "${COMPLETED[$stem]:-}" ]]; then',
        '        echo "[$TOTAL] SKIP: $name (already completed)"',
        '        SKIPPED=$((SKIPPED + 1))',
        "        return 0",
        "    fi",
        "    ",
        '    echo "[$TOTAL] Scanning: $name"',
        "    ",
        "    # Run JSON scan with log capture",
        '    if "$SCANNER" scan "$path" \\',
        "        --use-behavioral --use-trigger --use-llm --enable-meta \\",
        '        --format json -o "$RESULTS_DIR/${stem}-scan.json" \\',
        '        2>&1 | tee "$RESULTS_DIR/${stem}-scan.log"; then',
        "        ",
        "        # Run markdown scan",
        '        "$SCANNER" scan "$path" \\',
        "            --use-behavioral --use-trigger --use-llm --enable-meta \\",
        '            --format markdown -o "$RESULTS_DIR/${stem}-scan.md" 2>/dev/null',
        "        ",
        "        # Mark as completed",
        '        echo "$stem" >> "$PROGRESS_FILE"',
        '        SUCCESS=$((SUCCESS + 1))',
        '        echo "    OK"',
        "    else",
        '        FAILED=$((FAILED + 1))',
        '        echo "    FAILED (see $RESULTS_DIR/${stem}-scan.log)"',
        "    fi",
        "}",
        "",
    ]

    for skill in skills:
        path = skill["skill_path"]
        stem = skill["output_stem"]
        name = skill["skill_name"].replace('"', '\\"').replace("$", "\\$")
        reasons = ", ".join(skill["reasons"])
        lines.append(f"# {reasons}")
        lines.append(f'scan_skill "{path}" "{stem}" "{name}"')
        lines.append("")

    lines.extend([
        "# Summary",
        "echo",
        "echo '=============================='",
        "echo 'Re-scan Summary'",
        "echo '=============================='",
        'echo "Total:    $TOTAL"',
        'echo "Skipped:  $SKIPPED (already done)"',
        'echo "Success:  $SUCCESS"',
        'echo "Failed:   $FAILED"',
        "echo",
        'if [[ $FAILED -gt 0 ]]; then',
        '    echo "Some scans failed. Check logs in $RESULTS_DIR/*.log"',
        '    echo "Re-run this script to retry failed scans."',
        "fi",
        'if [[ $SUCCESS -gt 0 ]]; then',
        '    echo "Run: python scripts/generate_report.py -i $RESULTS_DIR"',
        '    echo "     bash scripts/build-site.sh"',
        "fi",
    ])

    output_path.write_text("\n".join(lines) + "\n")
    output_path.chmod(0o755)
    logger.info("Re-scan script written to %s", output_path)


def main():
    parser = argparse.ArgumentParser(
        description="Check scan quality and identify scans needing LLM/meta re-analysis",
    )
    parser.add_argument(
        "-r", "--results-dir",
        type=Path,
        default=Path("results"),
        help="Directory containing *-scan.json files (default: results)",
    )
    parser.add_argument(
        "--generate-script",
        type=Path,
        default=None,
        help="Write re-scan shell script to this path",
    )
    parser.add_argument(
        "--scanner",
        type=str,
        default=".venv/bin/skill-scanner",
        help="Path to skill-scanner for generated script (default: .venv/bin/skill-scanner)",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=logging.INFO,
        format=LOG_FORMAT,
        datefmt=LOG_DATE_FORMAT,
    )

    # Read and classify all scan JSONs
    classifications = []
    for json_file in sorted(args.results_dir.glob("*-scan.json")):
        if "summary-report" in json_file.name:
            continue
        try:
            with open(json_file) as f:
                data = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.warning("Skipping %s: %s", json_file, e)
            continue

        result = classify_scan(data)
        result["file"] = str(json_file)
        result["skill_name"] = data.get("skill_name", "")
        result["skill_path"] = data.get("skill_path", "")
        result["output_stem"] = json_file.stem.replace("-scan", "").lstrip(".")
        result["duration"] = data.get("scan_duration_seconds", 0)
        result["findings_count"] = data.get("findings_count", 0)
        classifications.append(result)

    # Summary
    total = len(classifications)
    needs_rescan = [c for c in classifications if c["needs_rescan"]]

    from collections import Counter
    meta_counts = Counter(c["meta_status"] for c in classifications)
    llm_counts = Counter(c["llm_status"] for c in classifications)

    print(f"\n{'='*60}")
    print(f"Scan Quality Report â€” {total} scans")
    print(f"{'='*60}\n")

    print("Meta Analysis:")
    for status in [REAL, SILENT_FAILURE, NOT_REQUESTED, INDETERMINATE]:
        count = meta_counts.get(status, 0)
        if count:
            print(f"  {status:20s} {count:4d}")

    print("\nLLM Analysis:")
    for status in [REAL, SILENT_FAILURE, NOT_REQUESTED, INDETERMINATE]:
        count = llm_counts.get(status, 0)
        if count:
            print(f"  {status:20s} {count:4d}")

    print(f"\nNeeds re-scan: {len(needs_rescan)}/{total}")

    if needs_rescan:
        print(f"\nSkills needing re-scan:")
        for c in needs_rescan[:20]:
            reasons = ", ".join(c["reasons"])
            print(f"  {c['skill_name']:40s} [{reasons}]")
        if len(needs_rescan) > 20:
            print(f"  ... and {len(needs_rescan) - 20} more")

    # Generate re-scan script
    if args.generate_script and needs_rescan:
        generate_rescan_script(
            needs_rescan, args.generate_script, scanner_path=args.scanner,
        )
        print(f"\nRe-scan script: {args.generate_script}")
        print(f"Run: bash {args.generate_script}")
    elif args.generate_script:
        print("\nNo scans need re-running.")


if __name__ == "__main__":
    main()
