[
  {
    "id": "why-we-built-this",
    "date": "2026-02-01",
    "title": "Exploring Security Testing Capabilities in the AI Agent Skills Ecosystem",
    "tags": ["overview", "research", "motivation", "supply-chain-security"],
    "excerpt": "AI agent skills execute with full permissions and minimal oversight. An academic study found 26% contain vulnerabilities. We're building an automated scanning pipeline to explore and monitor the risk landscape.",
    "body": "<p>AI agent skills are having a moment. These modular packages—containing instructions, scripts, and resources that extend what AI coding assistants can do—have exploded in popularity. Registries like <a href=\"https://skills.sh\">skills.sh</a> and <a href=\"https://clawhub.ai\">clawhub.ai</a> now host thousands of community-contributed skills, promising everything from automated code review to cloud deployments.</p><p>But with great extensibility comes great risk. Skills execute with the agent's full permissions, often with minimal human oversight. A malicious or poorly-written skill can exfiltrate credentials, inject commands, or manipulate the agent into taking harmful actions—all while appearing to be a helpful productivity tool.</p><p>This project is our small contribution to understanding and mitigating that risk.</p><h3>Research: 26% of Skills Have Issues</h3><p>The first large-scale academic study of agent skill security dropped in January 2026. Researchers analyzed over 31,000 skills from two major marketplaces and found the results \"concerning\":</p><blockquote><p><strong>\"26.1% of skills contain at least one potential vulnerability, with data exfiltration patterns (13.3%) and privilege escalation risks (11.8%) most prevalent. We identified 14 distinct vulnerability patterns across 8 functional categories.\"</strong></p><p>— Liu et al., <em>\"Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale\"</em> (<a href=\"https://arxiv.org/abs/2601.10338\">arXiv:2601.10338</a>)</p></blockquote><p>The study introduced a taxonomy of 14 vulnerability patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Crucially, they found that skills bundling executable scripts were <strong>2.12× more likely</strong> to contain vulnerabilities than instruction-only skills.</p><p>Their severity breakdown adds important nuance: only 5.2% of flagged skills showed high-severity patterns suggesting malicious intent. The majority (12.8%) exhibited low-severity issues like unpinned dependencies—poor hygiene rather than active threats. But even \"negligent\" vulnerabilities become exploitable when attackers find them first.</p><h3>Cisco's Open Source Skill Scanner</h3><p>Around the same time, Cisco's AI Threat and Security Research team released <a href=\"https://github.com/cisco-ai-defense/skill-scanner\">skill-scanner</a>, an open-source tool combining static analysis, behavioral dataflow tracking, and optional LLM-based semantic analysis.</p><p>Their motivation, articulated in an accompanying blog post, captures the core problem:</p><blockquote><p><strong>\"Unlike MCP servers (which are often remote services), skills are local file packages that get installed and loaded directly from disk. Local packages are still untrusted inputs, and some of the most damaging behavior can hide inside the files themselves.\"</strong></p><p>— Chang, Narajala &amp; Habler, <em>\"Personal AI Agents like OpenClaw Are a Security Nightmare\"</em> (<a href=\"https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare\">Cisco Blogs</a>)</p></blockquote><p>The scanner detects prompt injection, data exfiltration, command injection, and other threat patterns using YAML and YARA rules, with optional LLM analysis for semantic threats that evade pattern matching. It outputs SARIF for CI/CD integration and includes a meta-analyzer that reduces false positives by ~65% while maintaining threat detection.</p><h3>What's Here: Automated Scanning Pipeline and Results Monitoring</h3><p>The research paper gave us the \"what\"—a taxonomy and prevalence data. Cisco gave us the \"how\"—production-grade detection tooling.</p><p>That's what this project does.</p><p><a href=\"https://github.com/AppSecHQ/skill-scanner-test\"><strong>AppSecHQ/skill-scanner-test</strong></a> is an experimental pipeline that:</p><ol><li><strong>Crawls public skill registries</strong> (currently skills.sh and clawhub.ai)</li><li><strong>Runs Cisco's skill-scanner</strong> against each skill</li><li><strong>Publishes results</strong> as JSON and Markdown reports</li></ol><p>It's intentionally simple, and an evolving exploration project.</p><h3>Current Results</h3><table><thead><tr><th>Metric</th><th>Count</th></tr></thead><tbody><tr><td>Total Skills Scanned</td><td>62</td></tr><tr><td>Safe Skills</td><td>57 (92%)</td></tr><tr><td>Skills with Issues</td><td>5 (8%)</td></tr><tr><td>Critical Findings</td><td>4</td></tr><tr><td>High Findings</td><td>10</td></tr></tbody></table><p>Our 8% \"issues\" rate is notably lower than the paper's 26%. Why? Several factors:</p><ul><li><strong>Different registries</strong>: We scan skills.sh and clawhub.ai; the paper scanned skills.rest and skillsmp.com. Curation matters.</li><li><strong>Different thresholds</strong>: The paper flagged low-severity patterns (unpinned dependencies); we're reporting based on the scanner's default severity levels.</li><li><strong>Sample size</strong>: <em>Drastically</em> smaller sample set at this stage. 62 skills vs. 31,132. We're just getting started.</li></ul><p>When comparing high-severity findings only, the rates converge: the paper found 5.2% high-severity, we're seeing ~6.5% (4 critical out of 62). The ecosystem-wide risk profile appears consistent across registries.</p><h3>How This Complements Prior Work</h3><table><thead><tr><th>Work</th><th>Contribution</th><th>Gap</th></tr></thead><tbody><tr><td><strong>Research Paper</strong></td><td>Taxonomy, prevalence data, validation methodology</td><td>Snapshot in time; no ongoing monitoring</td></tr><tr><td><strong>Cisco Scanner</strong></td><td>Production detection tool</td><td>Requires manual invocation per skill</td></tr><tr><td><strong>This Project</strong></td><td>Automated pipeline + public results</td><td>Narrow registry coverage (for now)</td></tr></tbody></table><h3>What's Next</h3><p>This is early-stage and evolving. Potential future investigation includes:</p><ul><li><strong>Expanding registry coverage</strong>: Add skills.rest, skillsmp.com (the registries from the research paper), Anthropic Skills, and others</li><li><strong>Historical tracking</strong>: Monitor how vulnerability rates change over time as the ecosystem matures</li><li><strong>Validation benchmarking</strong>: Use the paper's ground-truth annotation dataset to measure scanner accuracy</li><li><strong>Taxonomy alignment</strong>: Map scanner findings to the paper's 14-pattern classification for research comparability, Map scanner findings to OWASP LLM Top 10, OWASP Agentic Top 10</li></ul><hr><p><em>This post is part of the AppSecHQ Skill Scanner Test project documentation. See the <a href=\"https://github.com/AppSecHQ/skill-scanner-test/blob/main/results/summary-report.md\">full scan results</a> for detailed findings.</em></p>"
  },
  {
    "id": "first-scan-results",
    "date": "2026-02-01",
    "title": "First Scan Results: 22 Skills, 39 Findings",
    "tags": ["results", "overview"],
    "excerpt": "An overview of our initial scan of the top 22 AI agent skills from skills.sh and clawhub.ai, revealing 3 unsafe skills and a dominant data exfiltration pattern.",
    "body": "<p>We ran Cisco's <code>skill-scanner</code> against the 22 most-installed AI agent skills sourced from <strong>skills.sh</strong> and <strong>clawhub.ai</strong>. The scan used five analyzers: static, behavioral, trigger, LLM (Claude Sonnet 4.5), and meta-validation.</p><h3>By the Numbers</h3><ul><li><strong>22</strong> skills scanned</li><li><strong>39</strong> total findings</li><li><strong>3</strong> skills flagged as unsafe (clickup, agent-browser, Agent Wallet)</li><li><strong>19</strong> skills passed clean or with only advisory-level findings</li></ul><h3>Severity Breakdown</h3><p>Critical findings accounted for just 3 of 39 (7.7%), but all three were concentrated in a single skill: <strong>clickup</strong>. This skill alone accounted for 14 findings -- 36% of the total -- including hardcoded API tokens detected by YARA rules, environment variable exfiltration patterns, and a <code>while True:</code> infinite loop.</p><p>The severity distribution was: 3 Critical, 8 High, 21 Medium, 7 Low. Medium findings dominated, largely because LLM-detected credential management issues were consistently rated Medium rather than High after meta-analyzer review.</p><h3>Category Dominance: Data Exfiltration</h3><p>Data exfiltration accounted for <strong>20 of 39 findings (51%)</strong>. This isn't surprising -- most AI agent skills need to call external APIs, and the scanner flags any pattern where environment variables are read and network calls are made. The more interesting signal is that the meta-analyzer distinguished between <em>legitimate API usage</em> (YouTube skills calling the YouTube API) and <em>genuinely risky patterns</em> (clickup harvesting arbitrary env vars alongside network requests).</p><h3>Source Comparison</h3><p>Skills from <strong>clawhub.ai</strong> (clickup, Agent Wallet) had the highest finding density. The clickup skill, a community-contributed ClickUp integration, was the clear outlier with 14 findings including all 3 Critical-severity issues. Skills from <strong>skills.sh</strong> were generally cleaner, though agent-browser had 8 findings including a bash tool restriction violation.</p><p>The Vercel-authored skills (frontend-design, react-best-practices, composition-patterns, react-native-skills) all passed with zero findings -- likely due to Vercel's internal review process before publishing.</p>"
  },
  {
    "id": "static-vs-llm-scanning",
    "date": "2026-02-01",
    "title": "Static-Only vs LLM-Enhanced Scanning",
    "tags": ["methodology", "comparison", "llm"],
    "excerpt": "Comparing what static analysis catches versus what Claude Sonnet 4.5 finds -- and why the difference matters for AI agent security.",
    "body": "<p>Not all 22 skills ran through the same analysis pipeline. 15 skills were scanned with static + behavioral + trigger analyzers only, while 7 skills also received LLM analysis (Claude Sonnet 4.5) plus meta-validation. Comparing the two groups reveals distinct detection profiles.</p><h3>Static Analysis: Pattern Matching at Scale</h3><p>Static analysis excels at catching concrete, deterministic patterns: hardcoded credentials (YARA rules), suspicious URLs, environment variable harvesting, missing manifest fields, and known dangerous code patterns like <code>while True:</code> loops or unsanitized user input in shell commands.</p><p>The two highest-finding skills -- <strong>clickup</strong> (14 findings) and <strong>agent-browser</strong> (8 findings) -- were scanned with static analysis only. Static analysis caught everything from YARA-detected credential harvesting to behavioral cross-file exfiltration patterns. These are the kinds of findings that don't require understanding <em>intent</em> -- the patterns are either present or they aren't.</p><p>Static-only scans completed in under 1 second (clickup: 0.8s, agent-browser: 18ms). The speed makes them suitable for CI/CD integration.</p><h3>LLM Analysis: Understanding Context</h3><p>LLM-enhanced scans took 23-31 seconds per skill (average: ~26s) -- roughly 30x slower than static analysis. But they detect a fundamentally different class of issues.</p><p>The LLM analyzer found credential management anti-patterns that static analysis missed: skills instructing users to store API keys in <code>~/.zshenv</code> without security guidance, incomplete trust model disclosures, and third-party data transmission risks. These are <em>architectural</em> and <em>documentation-level</em> issues, not code-level patterns.</p><p>For example, the <strong>subtitles</strong> skill was flagged for instructing users to export <code>TRANSCRIPT_API_KEY</code> in their shell config -- a common but risky pattern that could expose the key if the user commits their dotfiles to a public repo. Static analysis can't catch this because the instruction appears in natural language, not code.</p><h3>The Overlap Gap</h3><p>Interestingly, the two most dangerous skills (clickup and agent-browser) did <em>not</em> receive LLM analysis in this scan run. This means the current results may undercount the full risk profile of those skills -- LLM analysis might surface additional contextual findings on top of the static detections. A follow-up scan with full analyzer coverage on all 22 skills would be valuable.</p><h3>Cost-Benefit</h3><p>For a CI/CD gate or quick triage, static-only scanning covers the high-signal patterns in under a second. For a thorough security review -- especially of skills that handle credentials or make API calls -- the 25-second investment in LLM analysis pays off by catching design-level issues that no regex can find.</p>"
  },
  {
    "id": "meta-analyzer-validation",
    "date": "2026-02-01",
    "title": "Meta-Analyzer Validation: Reducing False Positives",
    "tags": ["methodology", "meta-analyzer", "false-positives"],
    "excerpt": "How the meta-analyzer reviews LLM findings for legitimacy, downgrades false alarms, and adds exploitability context.",
    "body": "<p>The meta-analyzer runs as a second-pass review of LLM findings, using a separate LLM call to validate each finding against the original skill source. Every LLM finding in our scan was meta-validated -- here's what that process reveals.</p><h3>What Meta-Validation Does</h3><p>For each LLM finding, the meta-analyzer produces:</p><ul><li><strong>Validated</strong> (boolean): whether the finding is confirmed</li><li><strong>Confidence</strong>: HIGH, MEDIUM, or LOW</li><li><strong>Confidence Reason</strong>: explanation of the assessment</li><li><strong>Exploitability</strong>: how easily the issue could be exploited</li><li><strong>Impact</strong>: what happens if it is exploited</li></ul><h3>Confidence Distribution</h3><p>Across our scan, meta-confidence scores broke down as:</p><ul><li><strong>HIGH confidence</strong>: ~60% of validated findings. These are well-known anti-patterns the meta-analyzer confirms without hesitation, like API key exposure in setup instructions.</li><li><strong>MEDIUM confidence</strong>: ~40% of validated findings. These represent findings where the meta-analyzer recognizes legitimate functionality behind the flagged pattern.</li></ul><p>No findings were given LOW confidence, suggesting the LLM analyzer's initial detection quality is reasonably high -- it isn't generating findings the meta-analyzer outright rejects.</p><h3>The Downgrade Pattern</h3><p>The most valuable meta-analyzer behavior is <em>contextual downgrading</em>. Consider the <strong>Agent Wallet</strong> skill: the LLM flagged a missing <code>wallet.py</code> file as an unauthorized tool use risk (potential tool shadowing). The meta-analyzer validated the finding but assessed exploitability as <strong>Low</strong>, reasoning it was likely a \"documentation error rather than security threat.\" Without meta-validation, this finding might be triaged as a High-severity security issue requiring immediate action. With it, the team knows it's a documentation cleanup task.</p><p>Similarly, the Agent Wallet's social engineering finding -- about incomplete trust model disclosure regarding API key access -- was downgraded with the note: \"Clarity issue rather than deception.\" The meta-analyzer distinguishes between <em>malicious intent</em> and <em>poor documentation</em>, which is exactly the kind of nuance automated scanners traditionally lack.</p><h3>Exploitability Assessments</h3><p>The exploitability ratings add practical triage value:</p><ul><li><strong>\"Easy -- automatic on skill use\"</strong> (video-transcript data exfiltration): The API call happens as part of normal operation. This is expected behavior, not a bug.</li><li><strong>\"Medium -- Requires user to follow insecure instructions\"</strong> (youtube-playlist API key setup): The risk depends on user behavior, not a code flaw.</li><li><strong>\"Low -- Would require attacker to inject a malicious wallet.py file\"</strong> (agent-wallet tool shadowing): Exploitation requires a separate attack vector.</li></ul><h3>Limitations</h3><p>The meta-analyzer validated 100% of LLM findings in our scan -- it never fully rejected a finding. This could mean the LLM analyzer has high precision, or that the meta-analyzer has a bias toward confirmation. A larger sample with deliberately benign patterns would help calibrate the false-positive rate. Also, meta-validation adds a second LLM call per finding, roughly doubling the cost and latency of LLM-enhanced scanning.</p>"
  },
  {
    "id": "scaling-to-62-skills",
    "date": "2026-02-01",
    "title": "Scaling to top 50 Skills: New Finding Categories Emerge",
    "tags": ["results", "skills.sh", "prompt-injection", "browser-use"],
    "excerpt": "Expanding from 22 to 62 (the top 50 from skills.sh plus our prior scans), skills revealed two new threat categories -- prompt injection and tool chaining abuse -- and surfaced browser-use as a high-risk skill class.",
    "body": "<p>We expanded our scan coverage from 22 skills to <strong>62</strong>, adding the top 50 skills from <strong>skills.sh</strong> alongside the original clawhub.ai skills. The results: 62 total findings across 5 unsafe skills, with two entirely new threat categories that didn't appear in the initial scan.</p><h3>The Numbers</h3><ul><li><strong>62</strong> skills scanned (up from 22)</li><li><strong>62</strong> total findings (up from 39)</li><li><strong>5</strong> unsafe skills (up from 3): clickup, agent-browser, <strong>browser-use</strong> (new), <strong>expo-api-routes</strong> (new), Agent Wallet</li><li><strong>57</strong> clean or advisory-only skills (92% safe rate)</li></ul><p>The safe rate held steady at 92% even as we tripled coverage, suggesting the initial 22-skill sample was representative.</p><h3>New Threat Categories</h3><p>Two categories appeared for the first time in this expanded scan:</p><p><strong>Prompt injection (4 findings)</strong> -- The LLM analyzer identified indirect prompt injection patterns where skills instruct the agent to fetch and act on external content without validation. The <strong>doc-coauthoring</strong> skill tells the agent to \"use the appropriate integration to fetch\" shared documents -- creating a transitive trust vector where malicious content in an external document could hijack the agent's behavior. The <strong>canvas-design</strong> skill uses a two-stage pattern where the agent generates a \"design philosophy\" and then follows it without re-validation, a subtler form of self-prompt-injection. These are classified under AITech-1.2 (Indirect Prompt Injection).</p><p><strong>Tool chaining abuse (2 findings)</strong> -- YARA rules detected patterns where skills reference both read and write HTTP methods (<code>GET, POST, PUT, DELETE</code>) in ways that suggest read-then-exfiltrate chains. Both findings came from <strong>expo-api-routes</strong>, though the meta-analyzer assessed the skill as fundamentally safe -- it's a documentation-only skill with no executable code. This is a case where static pattern matching over-triggers on educational content.</p><h3>browser-use: The Standout Risk</h3><p><strong>browser-use</strong> emerged as the most architecturally concerning skill in the expanded dataset, with 8 findings including HIGH-severity command injection and data exfiltration.</p><p>The core issue: browser-use provides an agent with full control over the user's Chrome browser -- including cookies, active sessions, and extensions -- via <code>shell=True</code> execution of browser automation commands. The LLM analyzer flagged this as a credential/session exposure risk (AITech-8.2.3: Data Exfiltration via Agent Tooling), noting that an attacker could craft instructions to navigate to malicious sites, extract session cookies, or exfiltrate authentication tokens.</p><p>The skill also has a transitive trust problem: it interacts with arbitrary web pages whose content the agent processes, creating an indirect prompt injection surface. A malicious page could contain instructions that manipulate the agent's behavior.</p><p>This is a fundamentally different risk profile from the clickup skill (which has bad credential hygiene but limited scope). browser-use grants the agent <em>ambient authority</em> over the user's entire browser session.</p><h3>expo-api-routes: False Alarm Study</h3><p><strong>expo-api-routes</strong> was flagged as CRITICAL due to a YARA rule matching <code>curl http://localhost:8081/api/hello</code> as command injection. But the meta-analyzer's overall assessment was \"SAFE -- purely educational markdown with no executable code.\" This is a documentation skill that teaches Expo API route patterns. The CRITICAL finding is a clear false positive from static pattern matching -- the <code>curl</code> command appears in a code example, not in executable code.</p><p>This case validates the meta-analyzer's value: without it, a CRITICAL finding would demand immediate triage. With it, the team can see the context and deprioritize accordingly.</p><h3>Category Breakdown at Scale</h3><p>At 62 findings, the category distribution is:</p><ul><li><strong>Data exfiltration: 24 (39%)</strong> -- still dominant, but dropped from 51% in the 22-skill scan as new categories diluted the share</li><li><strong>Policy violation: 11 (18%)</strong> -- mostly missing license fields and undeclared tool usage</li><li><strong>Command injection: 9 (15%)</strong> -- includes browser-use CLI injection and webapp-testing's <code>shell=True</code> subprocess</li><li><strong>Unauthorized tool use: 6 (10%)</strong> -- tool restriction mismatches and undefined integration references</li><li><strong>Prompt injection: 4 (6%)</strong> -- new category, all from LLM analyzer</li><li><strong>Social engineering: 3 (5%)</strong> -- including canvas-design's instruction to overstate craftsmanship quality</li><li><strong>Tool chaining abuse: 2 (3%)</strong> -- new category, YARA-based detection</li><li><strong>Resource abuse: 2 (3%)</strong> -- infinite loops and missing referenced files</li><li><strong>Hardcoded secrets: 1 (2%)</strong> -- clickup's exported API token</li></ul><h3>What's Next</h3><p>The prompt injection findings are the most interesting signal from this expansion. They represent a threat class that's unique to AI agent skills -- traditional software doesn't have the concept of an LLM following instructions embedded in external content. As skill ecosystems grow, transitive trust and indirect prompt injection will likely become the dominant attack surface. We plan to expand coverage to additional registries and track how these patterns evolve.</p>"
  }
]
